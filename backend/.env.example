# Backend AI provider configuration

# Choose provider: cloudflare | openai | groq | deepseek | gateway
AI_PROVIDER=cloudflare

# Default models
DEFAULT_AI_MODEL=@cf/meta/llama-3.1-8b-instruct
DEFAULT_EMBEDDING_MODEL=@cf/baai/bge-m3
# Optional model whitelist (comma separated)
ALLOWED_AI_MODELS=@cf/meta/llama-3.1-8b-instruct,@cf/baai/bge-m3

# Cloudflare Workers AI
CLOUDFLARE_ACCOUNT_ID=your_account_id
CLOUDFLARE_API_TOKEN=your_api_token

# OpenAI
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL=https://api.openai.com/v1

# Groq (OpenAI-compatible)
GROQ_API_KEY=your_groq_key
GROQ_BASE_URL=https://api.groq.com/openai/v1

# DeepSeek (OpenAI-compatible)
DEEPSEEK_API_KEY=your_deepseek_key
DEEPSEEK_BASE_URL=https://api.deepseek.com

# AI Gateway (optional)
AI_GATEWAY_API_KEY=your_gateway_key
AI_GATEWAY_BASE_URL=https://gateway.example.com/openai/v1

# Server
PORT=3000
HOST=localhost
NODE_ENV=development

# Cost control & guardrails
# Max output tokens per request (hard cap)
AI_MAX_OUTPUT_TOKENS=512
# Daily max total AI calls (chat+embedding)
AI_DAILY_MAX_CALLS=2000
# Cache TTL for chat/completion results (seconds)
AI_CACHE_TTL_SECONDS=3600
# Cache TTL for embeddings (seconds)
AI_EMBED_CACHE_TTL_SECONDS=604800
# Max cache entries in memory
AI_CACHE_MAX_ENTRIES=1000