# 本地化书签爬取架构方案

## 🎯 核心目标

1. **隐私优先**：100% 客户端执行，零数据上传
2. **性能极致**：1000+ 书签流畅处理，不影响主流程
3. **体验优秀**：后台静默执行，渐进式数据呈现
4. **架构清晰**：队列、缓存、调度、持久化分层设计

---

## 📊 当前架构分析

### 问题诊断

```
当前流程：
  Serverless Crawler (优先) ❌ 上传数据到后端
         ↓ 失败
  Local Crawler (降级)  ⚠️ 直接用 DOMParser，未使用 Offscreen
         ↓
  Offscreen Document  ❌ 已实现但未使用
```

### 改进目标

```
新架构：
  Background/Management ← 用户触发
         ↓
  任务队列管理器 (Task Queue Manager)
         ↓
  Offscreen Document Worker ← 唯一爬取入口
         ↓
  多层缓存系统 (Memory + IndexedDB)
         ↓
  渐进式数据同步
```

---

## 🏗️ 新架构设计

### 整体架构图

```
┌─────────────────────────────────────────────────────────┐
│                    用户界面层 (UI)                        │
│  Management / Side Panel / Popup                        │
│  - 触发爬取任务                                          │
│  - 实时显示进度                                          │
│  - 渐进式数据呈现                                        │
└───────────────────────┬─────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────────────┐
│              任务调度层 (Task Scheduler)                 │
│  CrawlTaskScheduler                                     │
│  - 任务优先级队列 (PriorityQueue)                       │
│  - 空闲时段调度 (requestIdleCallback)                   │
│  - 并发控制 (每域名 1 个，全局 2-3 个)                  │
│  - 断点续爬 (持久化队列状态)                            │
└───────────────────────┬─────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────────────┐
│            Offscreen Document 执行层                     │
│  OffscreenCrawlerWorker (page-fetcher.js 增强版)        │
│  - 网络请求 (fetch + CORS 处理)                         │
│  - HTML 解析 (DOMParser in Offscreen)                   │
│  - Robots.txt 检查                                       │
│  - 域名级别限流 (1秒间隔)                               │
│  - 超时控制 (10秒)                                       │
└───────────────────────┬─────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────────────┐
│              多层缓存系统 (Cache Layer)                  │
│  L1: 内存缓存 (LRU, 500条, 5分钟)                       │
│  L2: IndexedDB 持久化缓存 (30天 TTL)                    │
│  - URL去重                                               │
│  - 版本管理                                              │
│  - 过期清理                                              │
└───────────────────────┬─────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────────────┐
│            数据同步层 (Data Sync Layer)                  │
│  - 增量同步到搜索索引                                    │
│  - 通知 UI 更新                                          │
│  - 统计数据更新                                          │
└─────────────────────────────────────────────────────────┘
```

---

## 🔧 核心组件设计

### 1. 任务队列管理器 (CrawlTaskScheduler)

#### 核心职责

- 任务入队/出队
- 优先级管理
- 并发控制
- 断点续爬
- 状态持久化

#### 数据结构

```typescript
interface CrawlTask {
  id: string // 任务ID (URL hash)
  url: string // 书签URL
  bookmarkId: string // 书签ID
  priority: number // 优先级 (0-100)
  status: 'pending' | 'running' | 'success' | 'failed' | 'paused'
  retryCount: number // 重试次数
  createdAt: number // 创建时间
  startedAt?: number // 开始时间
  finishedAt?: number // 完成时间
  error?: string // 错误信息
}

interface QueueState {
  tasks: CrawlTask[] // 任务列表
  runningTasks: Map<string, CrawlTask> // 运行中任务
  domainLastAccess: Map<string, number> // 域名最后访问时间
  statistics: {
    total: number // 总任务数
    completed: number // 已完成
    failed: number // 已失败
    pending: number // 待处理
  }
}
```

#### 优先级策略

```typescript
/**
 * 优先级计算公式
 * priority = baseScore + recencyBonus + accessBonus + manualBonus
 */
function calculatePriority(bookmark: Bookmark): number {
  let priority = 50 // 基础分数

  // 1. 最近添加的书签优先 (0-20分)
  const daysSinceAdded =
    (Date.now() - bookmark.dateAdded) / (1000 * 60 * 60 * 24)
  if (daysSinceAdded < 7) priority += 20
  else if (daysSinceAdded < 30) priority += 10

  // 2. 最近访问的书签优先 (0-20分)
  if (bookmark.dateLastUsed) {
    const daysSinceUsed =
      (Date.now() - bookmark.dateLastUsed) / (1000 * 60 * 60 * 24)
    if (daysSinceUsed < 1) priority += 20
    else if (daysSinceUsed < 7) priority += 10
  }

  // 3. 用户手动触发的优先 (0-10分)
  if (bookmark.manualTrigger) priority += 10

  return priority
}
```

#### 并发控制策略

```typescript
class ConcurrencyController {
  private readonly MAX_GLOBAL_CONCURRENT = 2 // 全局最多2个并发
  private readonly MAX_PER_DOMAIN_CONCURRENT = 1 // 每域名最多1个
  private readonly MIN_DOMAIN_INTERVAL_MS = 1000 // 域名间隔1秒

  private runningCount = 0
  private domainRunning = new Map<string, number>()
  private domainLastAccess = new Map<string, number>()

  canStartTask(url: string): boolean {
    const domain = new URL(url).hostname

    // 检查全局并发
    if (this.runningCount >= this.MAX_GLOBAL_CONCURRENT) return false

    // 检查域名并发
    if (
      (this.domainRunning.get(domain) || 0) >= this.MAX_PER_DOMAIN_CONCURRENT
    ) {
      return false
    }

    // 检查域名间隔
    const lastAccess = this.domainLastAccess.get(domain) || 0
    if (Date.now() - lastAccess < this.MIN_DOMAIN_INTERVAL_MS) return false

    return true
  }

  startTask(url: string): void {
    const domain = new URL(url).hostname
    this.runningCount++
    this.domainRunning.set(domain, (this.domainRunning.get(domain) || 0) + 1)
    this.domainLastAccess.set(domain, Date.now())
  }

  finishTask(url: string): void {
    const domain = new URL(url).hostname
    this.runningCount--
    this.domainRunning.set(domain, (this.domainRunning.get(domain) || 1) - 1)
  }
}
```

---

### 2. Offscreen Document 爬虫工作器

#### 增强的 page-fetcher.js

```javascript
/**
 * 🎯 本地化爬虫 - 完全基于 Offscreen Document
 *
 * 核心特性：
 * - 100% 客户端执行
 * - Offscreen 环境中解析 HTML
 * - 域名级别限流
 * - Robots.txt 尊重
 * - 超时保护
 * - 错误降级
 */

// === 配置常量 ===
const MIN_DOMAIN_INTERVAL_MS = 1000 // 1秒
const REQUEST_TIMEOUT_MS = 10000 // 10秒
const MAX_RETRIES = 2 // 最多重试2次

// === 域名访问记录 ===
const DOMAIN_LAST_ACCESS = new Map()
const ROBOTS_CACHE = new Map()

// === 1. 域名限流 ===
async function waitForDomainSlot(domain) {
  const last = DOMAIN_LAST_ACCESS.get(domain) || 0
  const now = Date.now()
  const diff = now - last

  if (diff < MIN_DOMAIN_INTERVAL_MS) {
    await new Promise(r => setTimeout(r, MIN_DOMAIN_INTERVAL_MS - diff))
  }

  DOMAIN_LAST_ACCESS.set(domain, Date.now())
}

// === 2. Robots.txt 检查 ===
async function checkRobotsTxt(url) {
  const domain = new URL(url).hostname

  // 检查缓存
  const cached = ROBOTS_CACHE.get(domain)
  if (cached && Date.now() - cached.timestamp < 24 * 60 * 60 * 1000) {
    return cached.allowed
  }

  try {
    const robotsUrl = `https://${domain}/robots.txt`
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), 5000)

    const response = await fetch(robotsUrl, {
      signal: controller.signal,
      headers: { 'User-Agent': 'AcuityBookmarks-Extension/1.0' }
    })

    clearTimeout(timeoutId)

    let allowed = true
    if (response.ok) {
      const text = await response.text()
      // 简化解析：检查是否有 User-agent: * + Disallow: /
      if (/User-agent:\s*\*/i.test(text) && /Disallow:\s*\//i.test(text)) {
        allowed = false
      }
    }

    // 缓存结果
    ROBOTS_CACHE.set(domain, { allowed, timestamp: Date.now() })
    return allowed
  } catch (error) {
    // 无法获取则默认允许
    ROBOTS_CACHE.set(domain, { allowed: true, timestamp: Date.now() })
    return true
  }
}

// === 3. 创建 Offscreen Document ===
async function ensureOffscreenDocument() {
  try {
    // 检查是否已存在
    if (chrome.offscreen && (await chrome.offscreen.hasDocument?.())) {
      return true
    }
  } catch (e) {
    // hasDocument 可能不存在，继续尝试创建
  }

  try {
    await chrome.offscreen.createDocument({
      url: 'offscreen.html',
      reasons: ['DOM_SCRAPING'],
      justification: 'Parse bookmark page metadata locally for privacy'
    })
    return true
  } catch (error) {
    console.error('[CrawlerWorker] Failed to create offscreen document:', error)
    return false
  }
}

// === 4. 使用 Offscreen 解析 HTML ===
async function parseHTMLInOffscreen(html, url) {
  await ensureOffscreenDocument()

  return new Promise((resolve, reject) => {
    const timeout = setTimeout(() => {
      reject(new Error('Offscreen parsing timeout'))
    }, 3000)

    chrome.runtime.sendMessage({ type: 'PARSE_HTML', html, url }, response => {
      clearTimeout(timeout)

      if (chrome.runtime.lastError) {
        reject(new Error(chrome.runtime.lastError.message))
        return
      }

      resolve(response || {})
    })
  })
}

// === 5. 降级：正则表达式解析 ===
function fallbackParse(html) {
  const extract = regex => {
    const match = html.match(regex)
    return match ? match[1].trim() : ''
  }

  return {
    title: extract(/<title[^>]*>([^<]*)<\/title>/i),
    description: extract(
      /<meta[^>]*name=["']description["'][^>]*content=["']([^"']+)["'][^>]*>/i
    ),
    keywords: extract(
      /<meta[^>]*name=["']keywords["'][^>]*content=["']([^"']+)["'][^>]*>/i
    ),
    ogTitle: extract(
      /<meta[^>]*property=["']og:title["'][^>]*content=["']([^"']+)["'][^>]*>/i
    ),
    ogDescription: extract(
      /<meta[^>]*property=["']og:description["'][^>]*content=["']([^"']+)["'][^>]*>/i
    ),
    ogImage: extract(
      /<meta[^>]*property=["']og:image["'][^>]*content=["']([^"']+)["'][^>]*>/i
    ),
    ogSiteName: extract(
      /<meta[^>]*property=["']og:site_name["'][^>]*content=["']([^"']+)["'][^>]*>/i
    )
  }
}

// === 6. 核心爬取函数 ===
export async function crawlBookmarkLocally(url, options = {}) {
  const {
    respectRobots = true,
    timeout = REQUEST_TIMEOUT_MS,
    retryCount = 0
  } = options

  const startTime = Date.now()
  const domain = new URL(url).hostname

  try {
    // Step 1: 域名限流
    await waitForDomainSlot(domain)

    // Step 2: Robots.txt 检查
    if (respectRobots) {
      const robotsAllowed = await checkRobotsTxt(url)
      if (!robotsAllowed) {
        return {
          success: false,
          error: 'Blocked by robots.txt',
          robotsAllowed: false,
          duration: Date.now() - startTime
        }
      }
    }

    // Step 3: 发起网络请求
    const controller = new AbortController()
    const timeoutId = setTimeout(() => controller.abort(), timeout)

    const response = await fetch(url, {
      method: 'GET',
      redirect: 'follow',
      signal: controller.signal,
      headers: {
        'User-Agent':
          'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        Accept:
          'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Cache-Control': 'no-cache'
      }
    })

    clearTimeout(timeoutId)

    // Step 4: 检查响应
    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`)
    }

    const contentType = response.headers.get('content-type') || ''
    if (!/text\/html|application\/xhtml\+xml/i.test(contentType)) {
      throw new Error(`Unsupported content type: ${contentType}`)
    }

    // Step 5: 读取 HTML
    const html = await response.text()

    // Step 6: 解析元数据（优先 Offscreen，失败则降级）
    let metadata
    try {
      metadata = await parseHTMLInOffscreen(html, url)
    } catch (offscreenError) {
      console.warn(
        '[CrawlerWorker] Offscreen parsing failed, fallback to regex:',
        offscreenError
      )
      metadata = fallbackParse(html)
    }

    // Step 7: 返回结果
    return {
      success: true,
      url: response.url || url, // 处理重定向
      httpStatus: response.status,
      metadata,
      robotsAllowed: true,
      duration: Date.now() - startTime
    }
  } catch (error) {
    // 重试逻辑
    if (retryCount < MAX_RETRIES && shouldRetry(error)) {
      console.log(
        `[CrawlerWorker] Retry ${retryCount + 1}/${MAX_RETRIES}: ${url}`
      )
      await new Promise(r => setTimeout(r, 1000 * (retryCount + 1)))
      return crawlBookmarkLocally(url, {
        ...options,
        retryCount: retryCount + 1
      })
    }

    // 返回失败结果
    return {
      success: false,
      url,
      error: error.message || String(error),
      errorType: classifyError(error),
      duration: Date.now() - startTime
    }
  }
}

// === 7. 错误分类 ===
function classifyError(error) {
  const message = error.message || String(error)

  if (message.includes('aborted') || message.includes('timeout')) {
    return 'timeout'
  }
  if (message.includes('CORS') || message.includes('blocked')) {
    return 'cors'
  }
  if (message.includes('Failed to fetch') || message.includes('NetworkError')) {
    return 'network'
  }
  if (message.includes('HTTP 4') || message.includes('HTTP 5')) {
    return 'http_error'
  }

  return 'unknown'
}

// === 8. 判断是否应该重试 ===
function shouldRetry(error) {
  const errorType = classifyError(error)
  return errorType === 'timeout' || errorType === 'network'
}

// === 9. 统计信息 ===
export function getCrawlerStats() {
  return {
    domainsCached: DOMAIN_LAST_ACCESS.size,
    robotsCached: ROBOTS_CACHE.size
  }
}

// === 10. 清理缓存 ===
export function clearCrawlerCache() {
  DOMAIN_LAST_ACCESS.clear()
  ROBOTS_CACHE.clear()
}
```

#### 增强的 offscreen.js

```javascript
/**
 * 🎯 Offscreen Document - HTML 解析工作器
 *
 * 职责：
 * - 使用真实 DOM 环境解析 HTML
 * - 提取完整的元数据
 * - 错误容错
 */

;(() => {
  // === 辅助函数：获取 meta 标签内容 ===
  function getMeta(doc, name) {
    const el1 = doc.querySelector(`meta[name="${name}"]`)
    const el2 = doc.querySelector(`meta[property="${name}"]`)
    return (
      el1?.getAttribute('content') ||
      el2?.getAttribute('content') ||
      ''
    ).trim()
  }

  // === 辅助函数：安全获取文本内容 ===
  function getTextContent(element) {
    return (element?.textContent || '').trim()
  }

  // === 核心解析函数 ===
  function parseHTML(html, url) {
    try {
      const parser = new DOMParser()
      const doc = parser.parseFromString(html, 'text/html')

      // 1. 基础元数据
      const title = getTextContent(doc.querySelector('title'))
      const description = getMeta(doc, 'description')
      const keywords = getMeta(doc, 'keywords')

      // 2. Open Graph 元数据
      const ogTitle = getMeta(doc, 'og:title')
      const ogDescription = getMeta(doc, 'og:description')
      const ogImage = getMeta(doc, 'og:image')
      const ogSiteName = getMeta(doc, 'og:site_name')
      const ogType = getMeta(doc, 'og:type')

      // 3. Twitter Card 元数据
      const twitterCard = getMeta(doc, 'twitter:card')
      const twitterTitle = getMeta(doc, 'twitter:title')
      const twitterDescription = getMeta(doc, 'twitter:description')
      const twitterImage = getMeta(doc, 'twitter:image')

      // 4. 网站图标
      const iconEl = doc.querySelector(
        'link[rel~="icon"], link[rel="shortcut icon"], link[rel="apple-touch-icon"]'
      )
      const iconHref = iconEl?.getAttribute('href') || ''

      // 5. 语言
      const lang =
        doc.documentElement.lang ||
        doc.querySelector('meta[http-equiv="content-language"]')?.content ||
        ''

      // 6. 作者
      const author = getMeta(doc, 'author')

      return {
        // 基础
        title,
        description,
        keywords,
        lang,
        author,

        // Open Graph
        ogTitle,
        ogDescription,
        ogImage,
        ogSiteName,
        ogType,

        // Twitter Card
        twitterCard,
        twitterTitle,
        twitterDescription,
        twitterImage,

        // 图标
        iconHref
      }
    } catch (error) {
      console.error('[Offscreen] Parse error:', error)
      return {}
    }
  }

  // === 监听消息 ===
  chrome.runtime.onMessage.addListener((msg, sender, sendResponse) => {
    if (msg?.type === 'PARSE_HTML') {
      try {
        const result = parseHTML(msg.html || '', msg.url || '')
        sendResponse(result)
      } catch (error) {
        console.error('[Offscreen] Message handler error:', error)
        sendResponse({})
      }
      return true // 保持消息通道打开
    }
  })

  console.log('[Offscreen] Worker initialized')
})()
```

---

### 3. 多层缓存系统

#### L1: 内存缓存 (LRU)

```typescript
/**
 * LRU 内存缓存
 * - 最多 500 条
 * - 每条 5 分钟 TTL
 * - 减少 IndexedDB 访问
 */
class LRUCache<K, V> {
  private cache = new Map<K, { value: V; timestamp: number }>()
  private readonly maxSize: number
  private readonly ttl: number

  constructor(maxSize = 500, ttl = 5 * 60 * 1000) {
    this.maxSize = maxSize
    this.ttl = ttl
  }

  get(key: K): V | undefined {
    const entry = this.cache.get(key)
    if (!entry) return undefined

    // 检查过期
    if (Date.now() - entry.timestamp > this.ttl) {
      this.cache.delete(key)
      return undefined
    }

    // LRU: 移到最后
    this.cache.delete(key)
    this.cache.set(key, entry)

    return entry.value
  }

  set(key: K, value: V): void {
    // 删除旧值
    this.cache.delete(key)

    // 如果满了，删除最老的（第一个）
    if (this.cache.size >= this.maxSize) {
      const firstKey = this.cache.keys().next().value
      this.cache.delete(firstKey)
    }

    // 添加新值
    this.cache.set(key, { value, timestamp: Date.now() })
  }

  has(key: K): boolean {
    return this.get(key) !== undefined
  }

  clear(): void {
    this.cache.clear()
  }

  size(): number {
    return this.cache.size
  }
}
```

#### L2: IndexedDB 持久化缓存

```typescript
/**
 * IndexedDB 缓存管理器
 * - 30天 TTL
 * - URL 去重
 * - 版本管理
 */
class CrawlCacheManager {
  private readonly DB_NAME = 'AcuityBookmarks_CrawlCache'
  private readonly STORE_NAME = 'crawlMetadata'
  private readonly TTL = 30 * 24 * 60 * 60 * 1000 // 30天

  private db: IDBDatabase | null = null
  private memoryCache = new LRUCache<string, CrawlMetadata>()

  async initialize(): Promise<void> {
    return new Promise((resolve, reject) => {
      const request = indexedDB.open(this.DB_NAME, 2)

      request.onerror = () => reject(request.error)
      request.onsuccess = () => {
        this.db = request.result
        resolve()
      }

      request.onupgradeneeded = event => {
        const db = (event.target as IDBOpenDBRequest).result

        // 创建对象存储
        if (!db.objectStoreNames.contains(this.STORE_NAME)) {
          const store = db.createObjectStore(this.STORE_NAME, {
            keyPath: 'url'
          })

          // 创建索引
          store.createIndex('bookmarkId', 'bookmarkId', { unique: false })
          store.createIndex('domain', 'domain', { unique: false })
          store.createIndex('lastCrawled', 'lastCrawled', { unique: false })
          store.createIndex('expiresAt', 'expiresAt', { unique: false })
        }
      }
    })
  }

  // 获取缓存（优先内存，其次 IndexedDB）
  async get(url: string): Promise<CrawlMetadata | null> {
    // L1: 检查内存缓存
    const cached = this.memoryCache.get(url)
    if (cached) {
      // 检查过期
      if (cached.expiresAt > Date.now()) {
        return cached
      }
    }

    // L2: 检查 IndexedDB
    const dbResult = await this.getFromDB(url)
    if (dbResult && dbResult.expiresAt > Date.now()) {
      // 回填内存缓存
      this.memoryCache.set(url, dbResult)
      return dbResult
    }

    return null
  }

  // 保存到缓存
  async set(metadata: CrawlMetadata): Promise<void> {
    // 设置过期时间
    metadata.expiresAt = Date.now() + this.TTL
    metadata.lastCrawled = Date.now()

    // L1: 保存到内存
    this.memoryCache.set(metadata.url, metadata)

    // L2: 保存到 IndexedDB
    await this.saveToDB(metadata)
  }

  // 批量获取
  async getMany(urls: string[]): Promise<Map<string, CrawlMetadata>> {
    const result = new Map<string, CrawlMetadata>()

    for (const url of urls) {
      const metadata = await this.get(url)
      if (metadata) {
        result.set(url, metadata)
      }
    }

    return result
  }

  // 清理过期数据
  async cleanExpired(): Promise<number> {
    if (!this.db) return 0

    return new Promise((resolve, reject) => {
      const transaction = this.db!.transaction([this.STORE_NAME], 'readwrite')
      const store = transaction.objectStore(this.STORE_NAME)
      const index = store.index('expiresAt')

      const range = IDBKeyRange.upperBound(Date.now())
      const request = index.openCursor(range)

      let count = 0
      request.onsuccess = () => {
        const cursor = request.result
        if (cursor) {
          cursor.delete()
          count++
          cursor.continue()
        } else {
          resolve(count)
        }
      }

      request.onerror = () => reject(request.error)
    })
  }

  private async getFromDB(url: string): Promise<CrawlMetadata | null> {
    if (!this.db) return null

    return new Promise(resolve => {
      const transaction = this.db!.transaction([this.STORE_NAME], 'readonly')
      const store = transaction.objectStore(this.STORE_NAME)
      const request = store.get(url)

      request.onsuccess = () => resolve(request.result || null)
      request.onerror = () => resolve(null)
    })
  }

  private async saveToDB(metadata: CrawlMetadata): Promise<void> {
    if (!this.db) return

    return new Promise((resolve, reject) => {
      const transaction = this.db!.transaction([this.STORE_NAME], 'readwrite')
      const store = transaction.objectStore(this.STORE_NAME)
      const request = store.put(metadata)

      request.onsuccess = () => resolve()
      request.onerror = () => reject(request.error)
    })
  }
}
```

---

### 4. 空闲调度器

```typescript
/**
 * 🎯 空闲调度器
 *
 * 策略：
 * - 使用 requestIdleCallback 在浏览器空闲时执行
 * - 检测用户活动，暂停爬取
 * - 页面可见性检测
 * - 电池状态检测（可选）
 */
class IdleScheduler {
  private isUserActive = false
  private lastActivity = Date.now()
  private readonly USER_IDLE_THRESHOLD = 30 * 1000 // 30秒

  constructor() {
    this.setupActivityDetection()
    this.setupVisibilityDetection()
  }

  // 设置活动检测
  private setupActivityDetection() {
    const events = ['mousedown', 'mousemove', 'keydown', 'scroll', 'touchstart']

    const onActivity = () => {
      this.isUserActive = true
      this.lastActivity = Date.now()
    }

    events.forEach(event => {
      document.addEventListener(event, onActivity, { passive: true })
    })

    // 定期检查用户是否空闲
    setInterval(() => {
      if (Date.now() - this.lastActivity > this.USER_IDLE_THRESHOLD) {
        this.isUserActive = false
      }
    }, 5000)
  }

  // 设置可见性检测
  private setupVisibilityDetection() {
    document.addEventListener('visibilitychange', () => {
      if (document.hidden) {
        // 页面隐藏时可以更激进地爬取
        this.isUserActive = false
      }
    })
  }

  // 请求空闲执行
  requestIdleExecution<T>(
    callback: () => Promise<T>,
    options?: { timeout?: number }
  ): Promise<T> {
    return new Promise((resolve, reject) => {
      // 如果用户活跃，等待空闲
      if (this.isUserActive) {
        if (typeof requestIdleCallback !== 'undefined') {
          requestIdleCallback(
            async deadline => {
              try {
                const result = await callback()
                resolve(result)
              } catch (error) {
                reject(error)
              }
            },
            { timeout: options?.timeout || 5000 }
          )
        } else {
          // 降级：使用 setTimeout
          setTimeout(async () => {
            try {
              const result = await callback()
              resolve(result)
            } catch (error) {
              reject(error)
            }
          }, 100)
        }
      } else {
        // 用户空闲，立即执行
        callback().then(resolve).catch(reject)
      }
    })
  }

  // 检查是否应该继续爬取
  shouldContinueCrawling(): boolean {
    // 用户活跃时暂停
    if (this.isUserActive) return false

    // 页面可见时降低优先级
    if (!document.hidden) {
      // 可以继续，但应该降低频率
      return Date.now() - this.lastActivity > this.USER_IDLE_THRESHOLD
    }

    // 页面隐藏时可以继续
    return true
  }
}
```

---

## 📈 性能优化策略

### 1. URL 去重

```typescript
/**
 * URL 标准化和去重
 */
function normalizeURL(url: string): string {
  try {
    const parsed = new URL(url)

    // 移除片段
    parsed.hash = ''

    // 移除常见的追踪参数
    const trackingParams = [
      'utm_source',
      'utm_medium',
      'utm_campaign',
      'ref',
      'source'
    ]
    trackingParams.forEach(param => {
      parsed.searchParams.delete(param)
    })

    // 排序查询参数（确保一致性）
    const sortedParams = Array.from(parsed.searchParams.entries()).sort(
      (a, b) => a[0].localeCompare(b[0])
    )

    parsed.search = new URLSearchParams(sortedParams).toString()

    return parsed.toString()
  } catch {
    return url
  }
}

/**
 * URL 分组（去重）
 */
function groupBookmarksByURL(
  bookmarks: chrome.bookmarks.BookmarkTreeNode[]
): Map<string, chrome.bookmarks.BookmarkTreeNode[]> {
  const groups = new Map<string, chrome.bookmarks.BookmarkTreeNode[]>()

  for (const bookmark of bookmarks) {
    if (!bookmark.url) continue

    const normalizedURL = normalizeURL(bookmark.url)

    if (!groups.has(normalizedURL)) {
      groups.set(normalizedURL, [])
    }

    groups.get(normalizedURL)!.push(bookmark)
  }

  return groups
}
```

### 2. 渐进式数据呈现

```typescript
/**
 * 🎯 渐进式数据同步
 *
 * 策略：
 * - 每爬取 N 个书签，就更新一次 UI
 * - 使用 postMessage 通知 UI 层
 * - 避免阻塞主线程
 */
class ProgressiveDataSync {
  private readonly UPDATE_THRESHOLD = 10 // 每 10 个更新一次
  private updateQueue: CrawlMetadata[] = []

  // 添加爬取结果
  add(metadata: CrawlMetadata) {
    this.updateQueue.push(metadata)

    if (this.updateQueue.length >= this.UPDATE_THRESHOLD) {
      this.flush()
    }
  }

  // 立即刷新
  async flush() {
    if (this.updateQueue.length === 0) return

    const batch = [...this.updateQueue]
    this.updateQueue = []

    // 1. 批量保存到缓存
    await Promise.all(batch.map(m => cacheManager.set(m)))

    // 2. 通知搜索索引更新
    await searchWorkerAdapter.updateDocuments(batch)

    // 3. 通知 UI 更新
    this.notifyUI({
      type: 'CRAWL_PROGRESS',
      data: batch.map(m => ({
        bookmarkId: m.bookmarkId,
        title: m.title,
        description: m.description
      }))
    })
  }

  private notifyUI(message: any) {
    // 通知所有打开的页面
    chrome.runtime.sendMessage(message).catch(() => {
      // 页面可能未打开，忽略错误
    })
  }
}
```

### 3. 断点续爬

```typescript
/**
 * 🎯 持久化队列状态
 *
 * 功能：
 * - 保存队列状态到 chrome.storage.local
 * - 扩展重启后恢复队列
 * - 支持暂停/继续
 */
class PersistentQueue {
  private readonly STORAGE_KEY = 'crawl_queue_state'

  // 保存队列状态
  async saveState(state: QueueState): Promise<void> {
    try {
      await chrome.storage.local.set({
        [this.STORAGE_KEY]: {
          tasks: Array.from(state.tasks),
          statistics: state.statistics,
          timestamp: Date.now()
        }
      })
    } catch (error) {
      console.error('[PersistentQueue] Failed to save state:', error)
    }
  }

  // 恢复队列状态
  async loadState(): Promise<QueueState | null> {
    try {
      const result = await chrome.storage.local.get(this.STORAGE_KEY)
      const saved = result[this.STORAGE_KEY]

      if (!saved) return null

      // 检查状态是否太旧（超过7天）
      if (Date.now() - saved.timestamp > 7 * 24 * 60 * 60 * 1000) {
        await this.clearState()
        return null
      }

      return {
        tasks: saved.tasks || [],
        runningTasks: new Map(),
        domainLastAccess: new Map(),
        statistics: saved.statistics || {
          total: 0,
          completed: 0,
          failed: 0,
          pending: 0
        }
      }
    } catch (error) {
      console.error('[PersistentQueue] Failed to load state:', error)
      return null
    }
  }

  // 清除队列状态
  async clearState(): Promise<void> {
    try {
      await chrome.storage.local.remove(this.STORAGE_KEY)
    } catch (error) {
      console.error('[PersistentQueue] Failed to clear state:', error)
    }
  }
}
```

---

## 🎯 完整使用示例

### 在 Management 页面触发批量爬取

```typescript
/**
 * 用户点击"刷新所有书签元数据"按钮
 */
async function handleRefreshAllMetadata() {
  // 1. 获取所有书签
  const allBookmarks = await chrome.bookmarks.getTree()
  const flatBookmarks = flattenBookmarkTree(allBookmarks)
  const urlBookmarks = flatBookmarks.filter(b => b.url)

  // 2. 显示确认对话框
  const confirmed = await showConfirmDialog({
    title: '刷新书签元数据',
    message: `将在后台爬取 ${urlBookmarks.length} 个书签的元数据，预计需要 ${estimateTime(urlBookmarks.length)} 分钟`,
    confirmText: '开始刷新',
    cancelText: '取消'
  })

  if (!confirmed) return

  // 3. 启动爬取任务
  const taskId = await crawlTaskScheduler.scheduleBookmarksCrawl(urlBookmarks, {
    priority: 'normal',
    onProgress: progress => {
      // 更新进度条
      updateProgressBar(progress)
    },
    onComplete: stats => {
      // 显示完成通知
      showNotification({
        title: '元数据刷新完成',
        message: `成功: ${stats.completed}, 失败: ${stats.failed}`,
        type: 'success'
      })
    }
  })

  // 4. 显示进度面板
  showProgressPanel({
    taskId,
    onPause: () => crawlTaskScheduler.pauseTask(taskId),
    onResume: () => crawlTaskScheduler.resumeTask(taskId),
    onCancel: () => crawlTaskScheduler.cancelTask(taskId)
  })
}

function estimateTime(count: number): number {
  // 考虑：每个书签平均 2 秒，并发 2 个，批次间隔 1.5 秒
  const avgTimePerBookmark = 2
  const concurrency = 2
  const batchInterval = 1.5
  const batchSize = 5

  const batches = Math.ceil(count / batchSize)
  const timeInSeconds =
    (count / concurrency) * avgTimePerBookmark + batches * batchInterval

  return Math.ceil(timeInSeconds / 60)
}
```

---

## 📊 性能指标

### 预期性能（1000条书签）

| 指标             | 数值     | 说明                   |
| ---------------- | -------- | ---------------------- |
| **去重后URL数**  | ~700     | 30%重复率              |
| **缓存命中率**   | 80%      | 30天缓存               |
| **实际爬取数**   | ~140     | 仅爬新增+过期          |
| **平均单次耗时** | 2秒      | 包含网络+解析          |
| **并发数**       | 2        | 全局限制               |
| **总耗时**       | ~2.5分钟 | 140 ÷ 2 × 2 + 批次间隔 |
| **内存占用**     | <50MB    | LRU + 流式处理         |
| **CPU占用**      | <5%      | 空闲调度               |

### 用户体验

- ✅ **主流程零影响**：后台静默执行
- ✅ **渐进式呈现**：每10个更新一次UI
- ✅ **可中断恢复**：支持暂停/继续
- ✅ **断点续爬**：扩展重启后自动恢复
- ✅ **智能限流**：用户活跃时自动降速

---

## 🔒 隐私保护

### 数据流向

```
用户书签 (本地)
    ↓
Chrome Extension (本地)
    ↓
Offscreen Document (本地)
    ↓
目标网站 (直连，HTTPS)
    ↓
HTML 响应 (本地处理)
    ↓
提取元数据 (本地)
    ↓
IndexedDB (本地存储)
    ↓
搜索索引 (本地)
```

### 隐私承诺

- ✅ **零数据上传**：所有数据本地处理
- ✅ **直连目标站**：不经过任何中间服务器
- ✅ **本地存储**：IndexedDB + chrome.storage.local
- ✅ **可审计**：开源代码，可验证
- ✅ **用户控制**：可随时暂停/清除

---

## 🚀 实施步骤

### Phase 1: 核心重构（1-2天）

1. ✅ 修改 `constants.ts`
   - `CRAWLER_CONFIG.MODE = 'local'`（强制本地模式）
2. ✅ 重构 `page-fetcher.js`
   - 实现完整的本地爬取逻辑
   - 增强 Offscreen 消息处理
3. ✅ 增强 `offscreen.js`
   - 提取更多元数据字段
   - 改进错误处理

### Phase 2: 队列系统（2-3天）

4. ✅ 创建 `CrawlTaskScheduler`
   - 优先级队列
   - 并发控制
   - 状态管理
5. ✅ 创建 `ConcurrencyController`
   - 全局限流
   - 域名限流
   - 间隔控制

### Phase 3: 缓存优化（1-2天）

6. ✅ 实现 `LRUCache`
   - 内存缓存层
7. ✅ 优化 `CrawlCacheManager`
   - 两层缓存架构
   - 过期清理

### Phase 4: 调度优化（1-2天）

8. ✅ 创建 `IdleScheduler`
   - 空闲检测
   - 活动检测
   - 可见性检测
9. ✅ 创建 `PersistentQueue`
   - 状态持久化
   - 断点续爬

### Phase 5: 集成测试（1-2天）

10. ✅ 修改 `lightweight-bookmark-enhancer.ts`
    - 移除 Serverless 调用
    - 使用新的本地爬取系统
11. ✅ UI 层集成
    - 进度显示
    - 控制面板
    - 通知提示

### Phase 6: 压力测试（1天）

12. ✅ 测试场景
    - 100条书签
    - 500条书签
    - 1000条书签
    - 5000条书签
13. ✅ 性能监控
    - 内存占用
    - CPU使用率
    - 网络流量
    - 用户感知延迟

---

## 📝 配置参数参考

### 推荐配置（.env）

```bash
# 爬虫模式（强制本地）
VITE_CRAWLER_MODE=local

# 并发控制
VITE_CRAWLER_CONCURRENCY=2
VITE_CRAWLER_PER_DOMAIN_CONCURRENCY=1

# 批量处理
VITE_CRAWLER_BATCH_SIZE=5
VITE_CRAWLER_BATCH_INTERVAL_MS=1500

# 频率限制
VITE_CRAWLER_DAILY_LIMIT=1000

# 调度策略
VITE_CRAWLER_USE_IDLE_SCHEDULING=true
VITE_CRAWLER_IDLE_DELAY_MS=3000

# Robots.txt
VITE_CRAWLER_RESPECT_ROBOTS=true
```

---

## 🎉 预期效果

### 技术指标

- ✅ **100% 本地化**：无任何数据上传
- ✅ **高性能**：1000书签 < 3分钟
- ✅ **低资源占用**：内存 < 50MB，CPU < 5%
- ✅ **高可靠性**：断点续爬，自动重试
- ✅ **用户友好**：渐进式呈现，可控制

### 用户体验

- ✅ **无感知**：后台自动执行
- ✅ **可感知**：实时进度反馈
- ✅ **可控制**：暂停/继续/取消
- ✅ **可信任**：隐私保护，数据本地

---

**文档版本**: v1.0  
**最后更新**: 2025-10-12  
**状态**: ✅ 待实施
